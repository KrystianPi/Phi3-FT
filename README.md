Instruction dataset -> Claude3.5/GPT4o dataset generation/enhancement -> phi3-medium-instruct (proxy) finetuning with hard predictions from GPT4o/Claude3.5 on runpod.io (1st ft mini on googlecolab) -> knowledge distillation from proxy model to phi3-mini-instruct.gguf -> finetune a reward model using synthetic dataset from GPT4o -> RLHF with the finetuned reward model -> serve on AWS Lambda (ECR, Jenkins, Load Balancer etc)
